#!/usr/bin/env python3
"""
Test Claude API integration
"""

import asyncio
import sys
from pathlib import Path
import argparse
from loguru import logger
from typing import List, Dict

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from app.core.llm_client import LLMClient
from app.core.config import settings


async def test_basic_completion():
    """Test basic text completion"""
    client = LLMClient()
    
    test_prompts = [
        {
            "prompt": "ÌïúÍµ≠Ïùò ÏàòÎèÑÎäî Ïñ¥ÎîîÏù∏Í∞ÄÏöî?",
            "type": "simple_lookup",
            "expected": "ÏÑúÏö∏"
        },
        {
            "prompt": "ÏÇºÏÑ±Ï†ÑÏûêÏùò 2023ÎÖÑ Îß§Ï∂úÏï°Ïù¥ 300Ï°∞ÏõêÏù¥ÏóàÎã§Î©¥, ÏòÅÏóÖÏù¥ÏùµÎ•† 10%Ïùº Îïå ÏòÅÏóÖÏù¥ÏùµÏùÄ ÏñºÎßàÏù∏Í∞ÄÏöî?",
            "type": "simple_lookup",
            "expected": "30Ï°∞Ïõê"
        },
        {
            "prompt": "Î∞òÎèÑÏ≤¥ ÏÇ∞ÏóÖÏùò ÏµúÍ∑º ÎèôÌñ•Í≥º Ìñ•ÌõÑ Ï†ÑÎßùÏóê ÎåÄÌï¥ Í∞ÑÎã®Ìûà ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.",
            "type": "standard",
            "expected": "Î∞òÎèÑÏ≤¥"
        }
    ]
    
    results = []
    
    for test in test_prompts:
        logger.info(f"Testing prompt: {test['prompt'][:50]}...")
        
        try:
            response = await client.generate_text(
                prompt=test['prompt'],
                question_type=test['type'],
                max_tokens=500,
                temperature=0.3
            )
            
            success = test['expected'].lower() in response.lower()
            
            results.append({
                "prompt": test['prompt'],
                "type": test['type'],
                "response": response[:200] + "..." if len(response) > 200 else response,
                "success": success,
                "error": None
            })
            
            logger.info(f"Response received: {'‚úÖ' if success else '‚ùå'}")
            
        except Exception as e:
            results.append({
                "prompt": test['prompt'],
                "type": test['type'],
                "response": None,
                "success": False,
                "error": str(e)
            })
            logger.error(f"Error: {str(e)}")
    
    return results


async def test_document_analysis():
    """Test document analysis capability"""
    client = LLMClient()
    
    # Sample financial document content
    sample_doc = """
    ÏÇºÏÑ±Ï†ÑÏûê 2024ÎÖÑ 3Î∂ÑÍ∏∞ Ïã§Ï†Å Î∞úÌëú
    
    1. Îß§Ï∂úÏï°: 79Ï°∞Ïõê (Ï†ÑÎÖÑ ÎèôÍ∏∞ ÎåÄÎπÑ 12% Ï¶ùÍ∞Ä)
    2. ÏòÅÏóÖÏù¥Ïùµ: 9.1Ï°∞Ïõê (ÏòÅÏóÖÏù¥ÏùµÎ•† 11.5%)
    3. ÎãπÍ∏∞ÏàúÏù¥Ïùµ: 7.2Ï°∞Ïõê
    
    Ï£ºÏöî ÏÇ¨ÏóÖÎ∂ÄÎ≥Ñ Ïã§Ï†Å:
    - Î∞òÎèÑÏ≤¥(DS): Îß§Ï∂ú 23Ï°∞Ïõê, ÏòÅÏóÖÏù¥Ïùµ 3.8Ï°∞Ïõê
    - Ïä§ÎßàÌä∏Ìè∞(IM): Îß§Ï∂ú 28Ï°∞Ïõê, ÏòÅÏóÖÏù¥Ïùµ 2.8Ï°∞Ïõê
    - ÎîîÏä§ÌîåÎ†àÏù¥(DP): Îß§Ï∂ú 7Ï°∞Ïõê, ÏòÅÏóÖÏù¥Ïùµ 0.7Ï°∞Ïõê
    
    Ï£ºÏöî ÏÑ±Í≥º:
    - HBM3 Î©îÎ™®Î¶¨ ÏñëÏÇ∞ Î≥∏Í≤©Ìôî
    - Í∞§Îü≠Ïãú S24 ÏãúÎ¶¨Ï¶à ÌåêÎß§ Ìò∏Ï°∞
    - OLED Ìå®ÎÑê ÏàòÏöî Ï¶ùÍ∞Ä
    """
    
    test_questions = [
        "ÏÇºÏÑ±Ï†ÑÏûêÏùò 3Î∂ÑÍ∏∞ Îß§Ï∂úÏï°ÏùÄ ÏñºÎßàÏù∏Í∞ÄÏöî?",
        "Î∞òÎèÑÏ≤¥ ÏÇ¨ÏóÖÎ∂ÄÏùò ÏòÅÏóÖÏù¥ÏùµÎ•†ÏùÑ Í≥ÑÏÇ∞Ìï¥Ï£ºÏÑ∏Ïöî.",
        "Ï†ÑÎÖÑ ÎåÄÎπÑ Îß§Ï∂ú ÏÑ±Ïû•Î•†ÏùÄ Î™á ÌçºÏÑºÌä∏Ïù∏Í∞ÄÏöî?"
    ]
    
    results = []
    
    for question in test_questions:
        logger.info(f"Testing document analysis: {question}")
        
        try:
            result = await client.analyze_document(
                document_content=sample_doc,
                question=question,
                doc_type="financial_report"
            )
            
            results.append({
                "question": question,
                "answer": result['answer'],
                "success": True,
                "error": None
            })
            
            logger.info("Analysis completed successfully")
            
        except Exception as e:
            results.append({
                "question": question,
                "answer": None,
                "success": False,
                "error": str(e)
            })
            logger.error(f"Error: {str(e)}")
    
    return results


async def test_model_routing():
    """Test model routing logic"""
    client = LLMClient()
    
    test_cases = [
        ("simple_lookup", 0.3, "simple"),
        ("standard", 0.5, "standard"),
        ("complex_analysis", 0.8, "advanced"),
        ("simple_lookup", 0.9, "simple"),  # Simple type overrides complexity
    ]
    
    results = []
    
    for question_type, complexity, expected_tier in test_cases:
        model = client.select_model(question_type, complexity)
        
        if "haiku" in model and expected_tier == "simple":
            correct = True
        elif "sonnet" in model and expected_tier == "standard":
            correct = True
        elif "opus" in model and expected_tier == "advanced":
            correct = True
        else:
            correct = False
        
        results.append({
            "question_type": question_type,
            "complexity": complexity,
            "selected_model": model,
            "expected_tier": expected_tier,
            "correct": correct
        })
        
        logger.info(f"Model routing: {question_type}/{complexity} -> {model} {'‚úÖ' if correct else '‚ùå'}")
    
    return results


async def test_rag_pipeline():
    """Test full RAG pipeline (requires indexed documents)"""
    from app.services.rag_pipeline import RAGPipeline
    from app.core.embedding_client import EmbeddingClient
    import chromadb
    from chromadb.config import Settings as ChromaSettings
    
    try:
        # Initialize components
        chromadb_client = chromadb.HttpClient(
            host="localhost",
            port=8000,
            settings=ChromaSettings(anonymized_telemetry=False)
        )
        llm_client = LLMClient()
        embedding_client = EmbeddingClient()
        
        rag_pipeline = RAGPipeline(chromadb_client, llm_client, embedding_client)
        
        # Test questions
        test_questions = [
            "ÏÇºÏÑ±Ï†ÑÏûêÏùò ÏµúÍ∑º Îß§Ï∂úÏùÄ ÏñºÎßàÏù∏Í∞ÄÏöî?",
            "LGÏ†ÑÏûêÏùò Ï£ºÏöî ÏÇ¨ÏóÖ Î∂ÑÏïºÎäî Î¨¥ÏóáÏù∏Í∞ÄÏöî?",
            "SKÌïòÏù¥ÎãâÏä§Ïùò HBM Ï†úÌíàÏóê ÎåÄÌï¥ ÏïåÎ†§Ï£ºÏÑ∏Ïöî."
        ]
        
        results = []
        
        for question in test_questions:
            logger.info(f"Testing RAG pipeline: {question}")
            
            try:
                answer, sources = await rag_pipeline.generate_response(
                    question=question,
                    top_k=3
                )
                
                results.append({
                    "question": question,
                    "answer": answer[:200] + "..." if len(answer) > 200 else answer,
                    "sources": sources,
                    "success": True,
                    "error": None
                })
                
                logger.info(f"Found {len(sources)} sources")
                
            except Exception as e:
                results.append({
                    "question": question,
                    "answer": None,
                    "sources": [],
                    "success": False,
                    "error": str(e)
                })
                logger.error(f"Error: {str(e)}")
        
        return results
        
    except Exception as e:
        logger.error(f"Failed to initialize RAG pipeline: {str(e)}")
        return []


def print_test_results(test_name: str, results: List[Dict]):
    """Print test results in a formatted way"""
    print(f"\n{'='*60}")
    print(f" {test_name}")
    print(f"{'='*60}")
    
    if not results:
        print("No results to display")
        return
    
    success_count = sum(1 for r in results if r.get('success', False))
    total_count = len(results)
    
    print(f"Success Rate: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)\n")
    
    for i, result in enumerate(results, 1):
        print(f"{i}. {'‚úÖ' if result.get('success') else '‚ùå'} ", end="")
        
        if 'prompt' in result:
            print(f"Prompt: {result['prompt'][:60]}...")
        elif 'question' in result:
            print(f"Question: {result['question'][:60]}...")
        elif 'question_type' in result:
            print(f"Type: {result['question_type']}, Complexity: {result['complexity']}")
        
        if result.get('error'):
            print(f"   Error: {result['error']}")
        elif 'response' in result and result['response']:
            print(f"   Response: {result['response']}")
        elif 'answer' in result and result['answer']:
            print(f"   Answer: {result['answer']}")
        elif 'selected_model' in result:
            print(f"   Model: {result['selected_model']}")
        
        if 'sources' in result and result['sources']:
            print(f"   Sources: {', '.join(result['sources'][:3])}")
        
        print()


async def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Test Claude API integration")
    parser.add_argument(
        "--test",
        choices=["basic", "document", "routing", "rag", "all"],
        default="basic",
        help="Type of test to run"
    )
    
    args = parser.parse_args()
    
    print(f"\nü§ñ Claude API Test Suite")
    print(f"API Key: {'‚úÖ Configured' if settings.CLAUDE_API_KEY else '‚ùå Missing'}")
    
    if not settings.CLAUDE_API_KEY:
        print("\n‚ùå Error: CLAUDE_API_KEY not found in environment variables")
        print("Please set your Claude API key in the .env file")
        return
    
    if args.test in ["basic", "all"]:
        logger.info("Running basic completion tests...")
        results = await test_basic_completion()
        print_test_results("Basic Completion Tests", results)
    
    if args.test in ["document", "all"]:
        logger.info("Running document analysis tests...")
        results = await test_document_analysis()
        print_test_results("Document Analysis Tests", results)
    
    if args.test in ["routing", "all"]:
        logger.info("Running model routing tests...")
        results = await test_model_routing()
        print_test_results("Model Routing Tests", results)
    
    if args.test in ["rag", "all"]:
        logger.info("Running RAG pipeline tests...")
        results = await test_rag_pipeline()
        if results:
            print_test_results("RAG Pipeline Tests", results)
        else:
            print("\n‚ö†Ô∏è  RAG pipeline tests require indexed documents.")
            print("   Run the indexing scripts first:")
            print("   1. python scripts/setup_file_system.py --with-samples")
            print("   2. python scripts/index_documents.py")
    
    print("\n‚úÖ Test suite completed!")


if __name__ == "__main__":
    asyncio.run(main())